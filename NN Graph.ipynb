{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from tensorflow.contrib.rnn import MultiRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(indata, out_channels, batch_norm=True, k=1, padding='SAME', stride=1):\n",
    "    strides = [1, stride, stride, 1]\n",
    "    in_channels = indata.get_shape()[-1]\n",
    "    shape = [1, k, in_channels, out_channels]\n",
    "    W = tf.get_variable('weights', shape=shape)\n",
    "    conv_out = tf.nn.conv2d(indata, W, strides=strides, padding=padding)\n",
    "    if batch_norm:\n",
    "        with tf.variable_scope('batch_normalization'):\n",
    "            mean, variance = tf.nn.moments(conv_out, [0, 1, 2])\n",
    "            scale = tf.get_variable('scale', shape=[out_channels])\n",
    "            offset = tf.get_variable('offset', shape=[out_channels])\n",
    "            conv_out = tf.nn.batch_normalization(conv_out, \n",
    "                                                 mean=mean, \n",
    "                                                 variance=variance, \n",
    "                                                 scale=scale, \n",
    "                                                 offset=offset,\n",
    "                                                 variance_epsilon=1e-5)\n",
    "    return conv_out\n",
    "\n",
    "def residual_layer(indata, out_channels, batch_norm=True, stride=1):\n",
    "    in_channel = indata.get_shape()[-1]\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_layer(indata, out_channels, batch_norm)\n",
    "    \n",
    "    with tf.variable_scope('conv2'):\n",
    "        with tf.variable_scope('a'):\n",
    "            conv2 = conv_layer(indata, out_channels)\n",
    "            conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        with tf.variable_scope('b'):\n",
    "            conv2 = conv_layer(conv2, out_channels, k=3)\n",
    "            conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        with tf.variable_scope('c'):\n",
    "            conv2 = conv_layer(conv2, out_channels)\n",
    "            conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    return tf.nn.relu(conv1 + conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checking files: 100%|██████████| 34383/34383 [00:16<00:00, 2097.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29849 files have labels, 23880 for training and 5969 for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from fast5_input import fast5batches\n",
    "\n",
    "batch_size = 200\n",
    "segment_length = 400\n",
    "\n",
    "f5b = fast5batches(batch_size=batch_size, \n",
    "                   segment_length=segment_length, \n",
    "                   fast5dir='../chiron-otrain/pass', \n",
    "                   training=True, \n",
    "                   test_ratio=.2,\n",
    "                   overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:35,  3.08it/s]\n",
      "200it [00:31,  3.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], shape=(0, 400, 3), dtype=float64),\n",
       " array([], dtype=int64),\n",
       " Labels(indices=array([], shape=(0, 2), dtype=int32), values=array([], dtype=int64)),\n",
       " array([], dtype=[('name', 'S142'), ('i', '<u4'), ('total', '<u4')]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f5b.next_batch(test=False, fill=True)\n",
    "f5b.next_batch(test=True, fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_features = f5b.raw_signals.shape[2]\n",
    "\n",
    "neurons_per_layer = 100\n",
    "layers = 4\n",
    "class_n = 5\n",
    "\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[batch_size, segment_length, num_features], name = 'x')\n",
    "sequence_length = tf.placeholder(tf.int32, shape=[batch_size], name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = num_features\n",
    "\n",
    "net = tf.reshape(x, [batch_size, 1, segment_length, in_channels])\n",
    "out_channel = 256\n",
    "\n",
    "for i in range(3):\n",
    "    with tf.variable_scope('residual_layer_'+str(i)):\n",
    "        net = residual_layer(net, out_channel, batch_norm=i==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_all = tf.reshape(net, [batch_size, segment_length, out_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/georg/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/georg/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "## first RNN layers ######################\n",
    "##########################################\n",
    "\n",
    "outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw = MultiRNNCell([LSTMCell(neurons_per_layer) for _ in range(layers)]), \n",
    "    cell_bw = MultiRNNCell([LSTMCell(neurons_per_layer) for _ in range(layers)]), \n",
    "    inputs = conv_all, \n",
    "    sequence_length = sequence_length,\n",
    "    dtype = tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'FF/logits:0' shape=(200, 400, 5) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('collapse'):\n",
    "\n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    outputs = tf.reshape(outputs, [batch_size, segment_length, 2, neurons_per_layer], name = 'outputs')\n",
    "\n",
    "    W_collapse = tf.get_variable('W_collapse', shape = [2, neurons_per_layer])\n",
    "    b_collapse = tf.get_variable('b_callapse', shape = [neurons_per_layer])\n",
    "\n",
    "    collapsed = tf.multiply(outputs, W_collapse)\n",
    "    collapsed = tf.reduce_sum(collapsed, axis=2)\n",
    "    collapsed = tf.nn.bias_add(collapsed, b_collapse)\n",
    "    collapsed = tf.reshape(collapsed, [batch_size*segment_length,neurons_per_layer], name = 'collapsed')\n",
    "\n",
    "with tf.name_scope('FF'):\n",
    "    \n",
    "    W_last = tf.get_variable('W_last', shape = [neurons_per_layer, class_n])\n",
    "    b_last = tf.get_variable('b_last', shape = [class_n])\n",
    "\n",
    "    logits = tf.matmul(collapsed, W_last)\n",
    "    logits = tf.add(logits, b_last)\n",
    "    logits = tf.reshape(logits, [batch_size, segment_length, class_n], name=\"logits\")\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('labels'):\n",
    "    indices = tf.placeholder(tf.int64, name='indices')\n",
    "    y = tf.placeholder(tf.int32, name='values')\n",
    "    dense_shape = (batch_size, segment_length)\n",
    "\n",
    "    labels = tf.SparseTensor(indices = indices, \n",
    "                             values = y, \n",
    "                             dense_shape = dense_shape\n",
    "                            )\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.ctc_loss(labels = labels, \n",
    "                       inputs = logits, \n",
    "                       sequence_length = sequence_length, \n",
    "                       time_major=False\n",
    "                      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#max_steps = 20000\n",
    "#init_rate = 1e-3\n",
    "global_step = tf.get_variable('global_step', trainable=False, shape=(),\n",
    "                              dtype=tf.int32,\n",
    "                              initializer=tf.zeros_initializer())\n",
    "#boundaries = [21000, 31000]\n",
    "#lr_values = [1e-4, 1e-5, 1e-6]\n",
    "#learning_rate = tf.train.piecewise_constant(global_step, boundaries, lr_values)\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "step = optimizer.minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('error'):\n",
    "    logits_transposed = tf.transpose(logits, perm=[1, 0, 2])\n",
    "    #print(logits)\n",
    "    #decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(logits_transposed, sequence_length, merge_repeated=True)\n",
    "    decoded, neg_sum_logits = tf.nn.ctc_beam_search_decoder(\n",
    "                logits_transposed,\n",
    "                sequence_length,\n",
    "                merge_repeated=False,\n",
    "                top_paths=1,\n",
    "                beam_width=30)\n",
    "\n",
    "    first_path = decoded[0]\n",
    "    edit_d = tf.edit_distance(tf.to_int32(first_path), labels, normalize=True)\n",
    "    error = tf.reduce_mean(edit_d, axis=0)\n",
    "    #err_4merge = tf.summary.scalar('Error_rate', error)\n",
    "    error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'log/save'\n",
    "\n",
    "#loss_4merge = tf.summary.scalar('loss', loss)\n",
    "#lr_4merge = tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "tf.summary.histogram('W_last', W_last)\n",
    "tf.summary.histogram('b_last', b_last)\n",
    "\n",
    "summary_train = tf.summary.merge([tf.summary.scalar('learning_rate', learning_rate), \n",
    "                                  tf.summary.scalar('loss', loss)])\n",
    "summary_test = tf.summary.merge([tf.summary.scalar('Error_rate', error)])\n",
    "\n",
    "\n",
    "#summary_all = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5830it [5:48:33,  3.59s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10312it [10:17:41,  3.59s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "14943it [14:55:54,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "19166it [19:09:24,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "24886it [24:52:27,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "29569it [29:32:37,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "34078it [34:03:28,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "38680it [38:39:47,  3.60s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "47260it [47:15:09,  3.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-917b682f118b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./logs/save/{}.ckpt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1619\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenvs/tensorflow36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if True:\n",
    "    i = 1\n",
    "    name = 'withall_exp-2_'+str(i)\n",
    "    #retrain = True\n",
    "    summary_path = 'logs/summary/' + name\n",
    "    lr_val = 10**-2\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.summary.FileWriter(summary_path, sess.graph)\n",
    "        try:\n",
    "            saver.restore(sess, './logs/save/{}.ckpt'.format(name))\n",
    "        except ValueError:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        #for _ in tqdm(range(2500)):\n",
    "        with tqdm() as bar:\n",
    "            while True:\n",
    "                X, ln, Y, fi = f5b.next_batch(test=False)\n",
    "                feed_dict = {x: X, \n",
    "                             sequence_length: ln, \n",
    "                             indices: Y.indices, \n",
    "                             y: Y.values,\n",
    "                             learning_rate: lr_val}\n",
    "                summary, _, gs_val, loss_val = sess.run([summary_train, step, global_step, loss], feed_dict = feed_dict)\n",
    "                summary_writer.add_summary(summary, global_step=gs_val)\n",
    "                summary_writer.flush()\n",
    "                bar.n = gs_val\n",
    "                bar.refresh()\n",
    "                if gs_val%10==0:\n",
    "                    X, ln, Y, fi = f5b.next_batch(test=True)\n",
    "                    feed_dict = {x: X, \n",
    "                                 sequence_length: ln, \n",
    "                                 indices: Y.indices, \n",
    "                                 y: Y.values,}\n",
    "                    summary, err_val, gs_val = sess.run([summary_test, error, global_step], feed_dict = feed_dict)\n",
    "                    summary_writer.add_summary(summary, global_step=gs_val)\n",
    "                    summary_writer.flush()\n",
    "                    saver.save(sess, './logs/save/{}.ckpt'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  glob import glob\n",
    "glob('./logs/save/withouttime_exp_-2*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if True:\n",
    "    with tf.Session() as sess:\n",
    "        name = 'withouttime_exp_-2_5'\n",
    "        saver.restore(sess, './logs/save/{}.ckpt-2500'.format(name))\n",
    "        temp = f5b.next_batch(test=False)\n",
    "        rs, ln, vals, ixs, fi = temp\n",
    "        feed_dict = {x:rs, \n",
    "                     sequence_length:ln}\n",
    "        #ed, fp, lgs = sess.run([edit_d, first_path, logits], feed_dict=feed_dict)\n",
    "        fp, lgs = sess.run([first_path, logits], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.values[fp.indices.T[0]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import jupyterthemes as jt\n",
    "%matplotlib inline\n",
    "jt.jtplot.style('onedork')\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "rs_list = []\n",
    "\n",
    "for ff in fi:\n",
    "    with h5py.File(ff['name'].decode(),'r') as input_data:\n",
    "        for read_name in input_data['Raw/Reads']:\n",
    "            s = segment_length*ff['i']\n",
    "            e = segment_length*(1+ff['i'])\n",
    "            rs_list.append(input_data['Raw/Reads'][read_name]['Signal'][s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.plot(raw_signals)\n",
    "#plt.step(x=np.arange(end-start),y=raw_signals[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgs -= np.min(lgs)\n",
    "lgs = np.exp(lgs)\n",
    "lgs /= np.sum(lgs,2)[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0,len(rs_list)-1)\n",
    "raw_signals = rs_list[i]\n",
    "l=90\n",
    "start = random.randint(0,len(raw_signals)-1-l)\n",
    "end = start+l\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex='all')\n",
    "\n",
    "\n",
    "\n",
    "im = axes[1].imshow(lgs[0,start:end].T, aspect='auto')\n",
    "axes[1].grid(False)\n",
    "\n",
    "ix = [['A','C','G','T',''][b] for b in lgs[0,start:end].argmax(axis=1)]\n",
    "\n",
    "axes[1].set_yticks(np.arange(5))\n",
    "axes[1].set_yticklabels(['A','C','G','T',r'$\\epsilon$'])\n",
    "\n",
    "fig.colorbar(im, ax=axes[1], orientation='horizontal')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[0].plot(raw_signals[start:end])\n",
    "axes[0].set_xticks(np.arange(end-start))\n",
    "axes[0].set_xticklabels(ix)\n",
    "\n",
    "for x,y,s in zip(np.arange(end-start), raw_signals[start:end], ix):\n",
    "    axes[0].text(x=x,y=y,s=s)\n",
    "\n",
    "\n",
    "jt.jtplot.style(None)\n",
    "plt.rcParams['figure.figsize'] = (15,5)\n",
    "plt.savefig(\"logits.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vals[ixs.T[0]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp.values[fp.indices.T[0]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_n = lgs[0].argmax(axis=1)\n",
    "print(ix_n[ix_n!=4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(a):\n",
    "    r = []\n",
    "    last_el = None\n",
    "    for el in a:\n",
    "        if el==last_el:\n",
    "            continue\n",
    "        else:\n",
    "            r.append(el)\n",
    "            last_el = el\n",
    "    return np.array(r)\n",
    "def rm_4(a):\n",
    "    return a[a!=4]\n",
    "\n",
    "\n",
    "a = lgs[0].argmax(axis=1)\n",
    "#print(a)\n",
    "print(rm_4(collapse(a)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prona",
   "language": "python",
   "name": "prona"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
